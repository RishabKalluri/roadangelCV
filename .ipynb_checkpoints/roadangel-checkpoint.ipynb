{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63be30c-0ef2-45ce-b395-ae4e854d3b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mediapipe\n",
      "Version: 0.10.18\n",
      "Summary: MediaPipe is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, edge, cloud and the web.\n",
      "Home-page: https://github.com/google/mediapipe\n",
      "Author: The MediaPipe Authors\n",
      "Author-email: mediapipe@google.com\n",
      "License: Apache 2.0\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages\n",
      "Requires: absl-py, attrs, flatbuffers, jax, jaxlib, matplotlib, numpy, opencv-contrib-python, protobuf, sentencepiece, sounddevice\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8457e677-d346-4e1c-9f86-237aef0e530d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e658a0a-a117-45c5-a1ed-bc793002a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e09f4d65-7002-469e-9d59-596cef2d0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Mesh and Drawing Utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1, color=(0, 255, 0))\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def getLandmarks(image, face_mesh):\n",
    "    \"\"\"\n",
    "    Detects face landmarks in an image and calculates relative coordinates.\n",
    "    \"\"\"\n",
    "    # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "    #image.flags.writeable = False\n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "    \n",
    "    landmarks = []\n",
    "    relative_landmarks = []\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        for face in results.multi_face_landmarks:\n",
    "            for landmark in face.landmark:\n",
    "                x = landmark.x\n",
    "                y = landmark.y\n",
    "                \n",
    "                # Convert normalized coordinates to image pixels\n",
    "                shape = image.shape\n",
    "                relative_x = int(x * shape[1])  # shape[1] is the width\n",
    "                relative_y = int(y * shape[0])  # shape[0] is the height\n",
    "                relative_landmarks.append((relative_x, relative_y))\n",
    "            landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "    return landmarks, relative_landmarks, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8b88c28-ccca-420a-a8cd-b3bb1d3f2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(1)  # Use 0 for the default camera, or 1 for external cameras\n",
    "    cap.set(3, 640)  # Set width\n",
    "    cap.set(4, 420)  # Set height\n",
    "    cap.set(10, 100)  # Set brightness\n",
    "\n",
    "    # Initialize FaceMesh model\n",
    "    face_mesh = mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,  # Enables iris landmarks\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        # Capture webcam frames\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print('Ignoring empty camera frame.')\n",
    "            continue\n",
    "\n",
    "        # Flip the frame horizontally and convert BGR to RGB\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get landmarks and results\n",
    "        #landmarks, results = getLandmarks(rgb_frame, face_mesh)\n",
    "        landmarks, relative_landmarks, results = getLandmarks(rgb_frame, face_mesh)\n",
    "\n",
    "        frame_output = frame.copy()\n",
    "        #Get eye positions\n",
    "        if len(landmarks) > 0:\n",
    "            rightEyeImg = getRightEye(frame_output, landmarks)\n",
    "            rightEyeHeight, rightEyeWidth, _ = rightEyeImg.shape\n",
    "            \n",
    "            xRightEye, yRightEye, rightEyeWidth, rightEyeHeight = getRightEyeRect(frame_output, landmarks)\n",
    "            cv2.rectangle(frame_output, (xRightEye, yRightEye),\n",
    "                          (xRightEye + rightEyeWidth, yRightEye + rightEyeHeight), (200, 21, 36), 2)\n",
    "            \n",
    "            # LEFT EYE\n",
    "            leftEyeImg = getLeftEye(frame_output, landmarks)\n",
    "            leftEyeHeight, leftEyeWidth, _ = leftEyeImg.shape\n",
    "            \n",
    "            xLeftEye, yLeftEye, leftEyeWidth, leftEyeHeight = getLeftEyeRect(frame_output, landmarks)\n",
    "            cv2.rectangle(frame_output, (xLeftEye, yLeftEye),\n",
    "                          (xLeftEye + leftEyeWidth, yLeftEye + leftEyeHeight), (200, 21, 36), 2)\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "                #drawing irisis\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=frame_output,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "                )\n",
    "            \n",
    "                # Draw irises\n",
    "                draw_iris(frame_output, face_landmarks.landmark, RIGHT_IRIS, (0, 255, 0))  # Right iris\n",
    "                draw_iris(frame_output, face_landmarks.landmark, LEFT_IRIS, (0, 0, 255))   # Left iris\n",
    "            \n",
    "            \n",
    "                #Gaze detection\n",
    "                image_shape = frame_output.shape\n",
    "\n",
    "                # Get bounding boxes for both eyes\n",
    "                right_eye_bbox = get_eye_bbox(face_landmarks.landmark, RIGHT_EYE, image_shape)\n",
    "                left_eye_bbox = get_eye_bbox(face_landmarks.landmark, LEFT_EYE, image_shape)\n",
    "\n",
    "                # Get iris landmarks\n",
    "                right_iris = [face_landmarks.landmark[i] for i in RIGHT_IRIS]\n",
    "                left_iris = [face_landmarks.landmark[i] for i in LEFT_IRIS]\n",
    "\n",
    "                # Detect gaze for each eye\n",
    "                right_gaze = detect_gaze(right_iris, right_eye_bbox,image_shape)\n",
    "                left_gaze = detect_gaze(left_iris, left_eye_bbox,image_shape)\n",
    "\n",
    "                # Draw bounding boxes and annotate gaze direction\n",
    "                cv2.rectangle(frame_output, (right_eye_bbox[0], right_eye_bbox[1]),\n",
    "                              (right_eye_bbox[0] + right_eye_bbox[2], right_eye_bbox[1] + right_eye_bbox[3]),\n",
    "                              (0, 255, 0), 2)\n",
    "                cv2.rectangle(frame_output, (left_eye_bbox[0], left_eye_bbox[1]),\n",
    "                              (left_eye_bbox[0] + left_eye_bbox[2], left_eye_bbox[1] + left_eye_bbox[3]),\n",
    "                              (0, 255, 0), 2)\n",
    "\n",
    "                cv2.putText(frame_output, f\"Right Eye: {right_gaze}\", (50, 50), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                cv2.putText(frame_output, f\"Left Eye: {left_gaze}\", (50, 100), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "        # Draw face mesh on the frame\n",
    "        #output_frame = drawFaceMesh(frame, results)\n",
    "    \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('MediaPipe FaceMesh', frame_output)\n",
    "\n",
    "        # Press 'q' to exit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866413d6-34c2-4e66-858c-2f7220276c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b45c5065-6f7d-408d-85c0-fd0c8f0eb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "\n",
    "def draw_iris(image, landmarks, indices, color):\n",
    "    \"\"\"\n",
    "    Draws circles for iris landmarks on the image.\n",
    "    \"\"\"\n",
    "    image_height, image_width, _ = image.shape\n",
    "    for idx in indices:\n",
    "        x = int(landmarks[idx].x * image_width)\n",
    "        y = int(landmarks[idx].y * image_height)\n",
    "        cv2.circle(image, (x, y), 2, color, -1)  # Draw small circles for the iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b35b1ef-8169-4164-a2a0-07e692ce12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drawFaceMesh(image, results):\n",
    "    \"\"\"\n",
    "    Draws face mesh landmarks on the image.\n",
    "    \"\"\"\n",
    "    image.flags.writeable = True\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=drawing_spec,\n",
    "                connection_drawing_spec=drawing_spec)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf3f1c63-7a24-4533-b093-7edcd647a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Iris and eye landmarks\n",
    "RIGHT_EYE = [33, 133, 160, 159, 158, 144, 153, 154, 155]\n",
    "LEFT_EYE = [362, 263, 387, 386, 385, 373, 380, 374, 381]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "\n",
    "def get_eye_bbox(landmarks, indices, image_shape):\n",
    "    \"\"\"\n",
    "    Get the bounding box of the eye based on landmarks.\n",
    "    \"\"\"\n",
    "    x_coords = [landmarks[i].x * image_shape[1] for i in indices]\n",
    "    y_coords = [landmarks[i].y * image_shape[0] for i in indices]\n",
    "\n",
    "    x_min = int(min(x_coords))\n",
    "    y_min = int(min(y_coords))\n",
    "    x_max = int(max(x_coords))\n",
    "    y_max = int(max(y_coords))\n",
    "\n",
    "    return (x_min, y_min, x_max - x_min, y_max - y_min)  # x, y, width, height\n",
    "\n",
    "def detect_gaze(iris_landmarks, eye_bbox, image_shape):\n",
    "    \"\"\"\n",
    "    Detect gaze direction with more granular descriptions based on iris position.\n",
    "    \"\"\"\n",
    "    # Convert normalized iris landmarks to pixel coordinates\n",
    "    iris_center_x = sum([landmark.x for landmark in iris_landmarks]) / len(iris_landmarks) * image_shape[1]\n",
    "    iris_center_y = sum([landmark.y for landmark in iris_landmarks]) / len(iris_landmarks) * image_shape[0]\n",
    "\n",
    "    # Eye bounding box dimensions\n",
    "    eye_left = eye_bbox[0]\n",
    "    eye_right = eye_bbox[0] + eye_bbox[2]\n",
    "    eye_top = eye_bbox[1]\n",
    "    eye_bottom = eye_bbox[1] + eye_bbox[3]\n",
    "\n",
    "    # Debug bounding box and iris center\n",
    "    #print(f\"Eye BBox: Left={eye_left}, Right={eye_right}, Top={eye_top}, Bottom={eye_bottom}\")\n",
    "    #print(f\"Iris Center (in pixels): X={iris_center_x}, Y={iris_center_y}\")\n",
    "\n",
    "    # Relative position within the eye box\n",
    "    iris_x_relative = (iris_center_x - eye_left) / (eye_right - eye_left)\n",
    "    iris_y_relative = (iris_center_y - eye_top) / (eye_bottom - eye_top)\n",
    "\n",
    "    # Debug relative position\n",
    "    #print(f\"iris_x_relative: {iris_x_relative}, iris_y_relative: {iris_y_relative}\")\n",
    "\n",
    "    # Define thresholds for gaze zones\n",
    "    horizontal_thresholds = [0.4, 0.6]  # Left, center, right\n",
    "    vertical_thresholds = [0.4, 0.6]    # Up, center, down\n",
    "\n",
    "    # Determine horizontal gaze direction\n",
    "    if iris_x_relative < horizontal_thresholds[0]:\n",
    "        horizontal_gaze = \"Left\"\n",
    "    elif iris_x_relative > horizontal_thresholds[1]:\n",
    "        horizontal_gaze = \"Right\"\n",
    "    else:\n",
    "        horizontal_gaze = \"Center\"\n",
    "\n",
    "    # Determine vertical gaze direction\n",
    "    if iris_y_relative < vertical_thresholds[0]:\n",
    "        vertical_gaze = \"Up\"\n",
    "    elif iris_y_relative > vertical_thresholds[1]:\n",
    "        vertical_gaze = \"Down\"\n",
    "    else:\n",
    "        vertical_gaze = \"Center\"\n",
    "\n",
    "    # Combine horizontal and vertical directions for detailed gaze description\n",
    "    if horizontal_gaze == \"Center\" and vertical_gaze == \"Center\":\n",
    "        return \"Looking Straight Ahead\"\n",
    "    elif horizontal_gaze == \"Center\":\n",
    "        return f\"Looking {vertical_gaze}\"\n",
    "    elif vertical_gaze == \"Center\":\n",
    "        return f\"Looking {horizontal_gaze}\"\n",
    "    else:\n",
    "        return f\"Looking {vertical_gaze}-{horizontal_gaze}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10cdea8b-cd4c-4f6d-a5d0-fd9911d6987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def head_pose_estimation(face_landmarks, image_shape):\n",
    "    \"\"\"\n",
    "    Estimate head pose using facial landmarks and OpenCV.\n",
    "    \"\"\"\n",
    "    # Camera parameters\n",
    "    focal_length = image_shape[1]\n",
    "    center = (image_shape[1] // 2, image_shape[0] // 2)\n",
    "    camera_matrix = np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "    # Assume no lens distortion\n",
    "    dist_coeffs = np.zeros((4, 1))\n",
    "\n",
    "    # 3D model points of the face (average human face)\n",
    "    model_points = np.array([\n",
    "        (0.0, 0.0, 0.0),           # Nose tip\n",
    "        (0.0, -330.0, -65.0),      # Chin\n",
    "        (-225.0, 170.0, -135.0),   # Left eye corner\n",
    "        (225.0, 170.0, -135.0),    # Right eye corner\n",
    "        (-150.0, -150.0, -125.0),  # Left mouth corner\n",
    "        (150.0, -150.0, -125.0)    # Right mouth corner\n",
    "    ])\n",
    "\n",
    "    # 2D image points from facial landmarks\n",
    "    image_points = np.array([\n",
    "        (face_landmarks[1].x * image_shape[1], face_landmarks[1].y * image_shape[0]),    # Nose tip\n",
    "        (face_landmarks[152].x * image_shape[1], face_landmarks[152].y * image_shape[0]),  # Chin\n",
    "        (face_landmarks[33].x * image_shape[1], face_landmarks[33].y * image_shape[0]),   # Left eye corner\n",
    "        (face_landmarks[263].x * image_shape[1], face_landmarks[263].y * image_shape[0]), # Right eye corner\n",
    "        (face_landmarks[78].x * image_shape[1], face_landmarks[78].y * image_shape[0]),   # Left mouth corner\n",
    "        (face_landmarks[308].x * image_shape[1], face_landmarks[308].y * image_shape[0])  # Right mouth corner\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "    # Solve PnP for pose estimation\n",
    "    success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "        model_points, image_points, camera_matrix, dist_coeffs\n",
    "    )\n",
    "\n",
    "    # Convert rotation vector to rotation matrix\n",
    "    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "\n",
    "    # Calculate pitch, yaw, roll\n",
    "    pitch, yaw, roll = cv2.decomposeProjectionMatrix(np.hstack((rotation_matrix, translation_vector)))[6]\n",
    "\n",
    "    return pitch[0], yaw[0], roll[0]  # In degrees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b076b52-3b2a-4f95-bf60-48cccac3fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_head_pose_and_iris(yaw, pitch, iris_x_relative, iris_y_relative):\n",
    "    \"\"\"\n",
    "    Combine head pose (yaw, pitch) and iris position (relative to eye) to detect gaze direction.\n",
    "    Args:\n",
    "        yaw (float): Rotation around the vertical axis (left/right).\n",
    "        pitch (float): Rotation around the horizontal axis (up/down).\n",
    "        iris_x_relative (float): Relative iris x-position within the eye (0 = left, 1 = right).\n",
    "        iris_y_relative (float): Relative iris y-position within the eye (0 = top, 1 = bottom).\n",
    "    Returns:\n",
    "        str: Final gaze direction.\n",
    "    \"\"\"\n",
    "    # Thresholds for head pose\n",
    "    yaw_thresholds = (-15, 15)  # Left (<-15), Center (-15 to 15), Right (>15)\n",
    "    pitch_thresholds = (-10, 10)  # Up (<-10), Center (-10 to 10), Down (>10)\n",
    "\n",
    "    # Thresholds for iris position (relative to eye)\n",
    "    iris_horizontal_thresholds = (0.4, 0.6)  # Left (<0.4), Center (0.4 to 0.6), Right (>0.6)\n",
    "    iris_vertical_thresholds = (0.4, 0.6)    # Up (<0.4), Center (0.4 to 0.6), Down (>0.6)\n",
    "\n",
    "    # Step 1: Head pose classification\n",
    "    if yaw < yaw_thresholds[0]:\n",
    "        head_horizontal = \"Left\"\n",
    "    elif yaw > yaw_thresholds[1]:\n",
    "        head_horizontal = \"Right\"\n",
    "    else:\n",
    "        head_horizontal = \"Center\"\n",
    "\n",
    "    if pitch < pitch_thresholds[0]:\n",
    "        head_vertical = \"Up\"\n",
    "    elif pitch > pitch_thresholds[1]:\n",
    "        head_vertical = \"Down\"\n",
    "    else:\n",
    "        head_vertical = \"Center\"\n",
    "\n",
    "    # Step 2: Iris position classification\n",
    "    if iris_x_relative < iris_horizontal_thresholds[0]:\n",
    "        iris_horizontal = \"Left\"\n",
    "    elif iris_x_relative > iris_horizontal_thresholds[1]:\n",
    "        iris_horizontal = \"Right\"\n",
    "    else:\n",
    "        iris_horizontal = \"Center\"\n",
    "\n",
    "    if iris_y_relative < iris_vertical_thresholds[0]:\n",
    "        iris_vertical = \"Up\"\n",
    "    elif iris_y_relative > iris_vertical_thresholds[1]:\n",
    "        iris_vertical = \"Down\"\n",
    "    else:\n",
    "        iris_vertical = \"Center\"\n",
    "\n",
    "    # Step 3: Combine classifications\n",
    "    # Weighted logic: prioritize iris for finer adjustments, head pose for larger movements\n",
    "    if head_horizontal == \"Center\" and iris_horizontal != \"Center\":\n",
    "        final_horizontal = iris_horizontal\n",
    "    else:\n",
    "        final_horizontal = head_horizontal\n",
    "\n",
    "    if head_vertical == \"Center\" and iris_vertical != \"Center\":\n",
    "        final_vertical = iris_vertical\n",
    "    else:\n",
    "        final_vertical = head_vertical\n",
    "\n",
    "    # Step 4: Return combined gaze direction\n",
    "    if final_horizontal == \"Center\" and final_vertical == \"Center\":\n",
    "        return \"Looking Straight Ahead\"\n",
    "    elif final_horizontal == \"Center\":\n",
    "        return f\"Looking {final_vertical}\"\n",
    "    elif final_vertical == \"Center\":\n",
    "        return f\"Looking {final_horizontal}\"\n",
    "    else:\n",
    "        return f\"Looking {final_vertical}-{final_horizontal}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f823db-a8c5-44b5-95f6-b6e83ade8708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
